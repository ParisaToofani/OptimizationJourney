{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add custom gradient calculation\n",
    "# Make it object oriented\n",
    "# amsgrad -> Done\n",
    "# maximize -> Done\n",
    "# Jitter -> Done\n",
    "import typing\n",
    "import numpy as np\n",
    "def AdamOptim(theta0,\n",
    "              objective_function_grad, \n",
    "              lr : float = 0.001,\n",
    "              m0 : float = 0.0, \n",
    "              v0 : float = 0.0,  \n",
    "              betta1 : float = 0.9, \n",
    "              betta2 : float = 0.999,  \n",
    "              epochs : int = 1000,\n",
    "              amsgrad = True,\n",
    "              vtHatmax : float = 0, \n",
    "              maximize = False):\n",
    "    \"\"\"\n",
    "    Requirement to start the algorithm:\n",
    "    lr: Step size (Learning rate)\n",
    "    Betta1 and betta2 -> Exponential decay rates for the moment estimates (usually in [0,1))\n",
    "    f(theta): Stochastic objective function with parameter theta\n",
    "    # Initializations variables:\n",
    "    theta0 -> Initial parameter vector\n",
    "    m0 -> Initialize first moment vector -> 0\n",
    "    v0 -> Initialize second moment vector -> 0\n",
    "    t -> Initialize timestep -> 0\n",
    "    epochs -> number of iterations\n",
    "    \"\"\"\n",
    "    # Start Algorithm\n",
    "    # Goal --> This is one dimentional (I have to apply it to higher dimentions)\n",
    "    jitter = 1e-8\n",
    "    theta = np.zeros((epochs, 1))   # The init vector of theta\n",
    "    gt = np.zeros((epochs, 1))      # The init vector of gradient\n",
    "    mt = np.zeros((epochs, 1))      # The init vector of first moment\n",
    "    vt = np.zeros((epochs, 1))      # The init vector of second moment\n",
    "    mtHat = np.zeros((epochs, 1))   # The init vector of modified first moment\n",
    "    vtHat = np.zeros((epochs, 1))   # The init vector of modified second moment\n",
    "    # Initialize the model\n",
    "    theta[0] = theta0\n",
    "    gt[0] = -objective_function_grad(theta[0]) if maximize else objective_function_grad(theta[0])\n",
    "    mt[0] = m0\n",
    "    vt[0] = v0\n",
    "    for t in range(1, epochs):\n",
    "        mt[t] = betta1 * mt[t-1] + (1 - betta1) * gt[t-1] # Update biased first moment estimate\n",
    "        vt[t] = betta2 * vt[t-1] + (1 - betta2) * np.dot(gt[t-1], gt[t-1]) # Update biased second raw moment estimate\n",
    "        mtHat[t] = mt[t] / (1 - betta1) # Compute bias-corrected first moment estimate\n",
    "        vtHat[t] = vt[t] / (1 - betta2) # Compute bias-corrected second raw moment estimate\n",
    "        # Start to update the input vector\n",
    "        if amsgrad:\n",
    "            vtHatmax = max(vtHat[t], vtHatmax)\n",
    "            theta[t] = theta[t-1] - lr * (mtHat[t]/(np.sqrt(vtHatmax) + jitter))\n",
    "        else:\n",
    "            theta[t] = theta[t-1] - lr * (mtHat[t]/(np.sqrt(vtHat[t]) + jitter))\n",
    "        # Update the gradient function\n",
    "        gt[t] = -objective_function_grad(theta[t]) if maximize else objective_function_grad(theta[t])\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(x):\n",
    "    return x**2\n",
    "\n",
    "def gradobjective(x):\n",
    "    return 2*x\n",
    "\n",
    "theta = AdamOptim(3, gradobjective)\n",
    "# Here, I can show it on "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etta\n",
    "# Delta\n",
    "# Initial Point\n",
    "# Grad\n",
    "# Epochs\n",
    "# This is AdaGrad with diagonal matrices\n",
    "def AdaGrad():\n",
    "    return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A basic example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develope a DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
